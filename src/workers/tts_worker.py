import os
import io
import re
import json
import asyncio
from datetime import datetime
from pydub import AudioSegment
import edge_tts
from PyQt5.QtCore import QThread, pyqtSignal

# --- ×”×’×“×¨×ª ×¡×•×’×™ ×©×’×™××•×ª ××•×ª×××™× ××™×©×™×ª ---
class ServerOverloadError(Exception):
    """×©×’×™××” ×©××¢×™×“×” ×©×”×©×¨×ª ×¢××•×¡ ××• ×“×•×—×” ××ª ×”×‘×§×©×”"""
    pass

class NetworkConnectionError(Exception):
    """×©×’×™××” ×©××¢×™×“×” ×¢×œ ×‘×¢×™×” ×‘××™× ×˜×¨× ×˜ ×”××§×•××™"""
    pass

class TTSWorker(QThread):
    progress_update = pyqtSignal(int)
    log_update = pyqtSignal(str)
    finished_success = pyqtSignal(str, list)
    finished_error = pyqtSignal(str)
    error = pyqtSignal(str)

    def __init__(self, text, output_file, voice, rate, volume, dicta_dict, parent=None, dual_mode=False):
        super().__init__(parent)
        self.text = text
        self.output_path = output_file
        self.voice = voice
        self.rate = rate
        self.speed = rate # ×ª×™×§×•×Ÿ: ××©×ª× ×” × ×“×¨×©
        self.volume = volume
        self.dicta_dict = dicta_dict
        self.dual_mode = dual_mode
        
        # ×’×™×‘×•×™ ×œ×§×•×œ×•×ª
        self.he_voice = voice
        self.en_voice = "en-US-AriaNeural"
        
        # ×”×’×“×¨×•×ª ×‘×¨×™×¨×ª ××—×“×œ ×œ×× ×™×¢×ª ×§×¨×™×¡×”
        self.user_max_limit = 15
        self.min_limit = 1
        self.current_limit = 15
        
        # ×©×œ×™×¤×ª ×”×’×“×¨×•×ª ××”×”×•×¨×” ×× ×§×™×™×
        self.settings = {}
        if parent and hasattr(parent, 'settings'):
            self.settings = parent.settings
            
            # 1. ×›××Ÿ ××ª×” ×©×•××‘ ××ª ×”-30 ××”×”×’×“×¨×•×ª
            self.user_max_limit = int(self.settings.get("max_concurrent", 5))
            
            # === 2. ×”×•×¡×£ ××ª ×”×©×•×¨×” ×”×–×•: ×¢×“×›×•×Ÿ ×”××’×‘×œ×” ×‘×¤×•×¢×œ ===
            self.current_limit = self.user_max_limit 
            # ===============================================

            if "selected_he_voice" in self.settings and hasattr(parent, 'he_voices'):
                 he_key = self.settings["selected_he_voice"]
                 self.he_voice = parent.he_voices.get(he_key, voice)
            
            if "selected_en_voice" in self.settings and hasattr(parent, 'en_voices'):
                 en_key = self.settings["selected_en_voice"]
                 self.en_voice = parent.en_voices.get(en_key, voice)

        # ××©×ª× ×™× ×¤× ×™××™×™×
        self.is_running = True
        self.loop = None
        self.results = {}
        self.skipped_sentences = []
        self.completed_count = 0
        self.total_sentences = 0
        self.processing_idxs = set() # ×ª×™×§×•×Ÿ: ×œ×× ×™×¢×ª ×§×¨×™×¡×” ×‘×œ×•×’×™×
        self.consecutive_successes = 0
        self.consecutive_errors = 0

    def run(self):
        print(f"\n=== STARTING TTS (Target: {self.user_max_limit}) ===")
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        try:
            self.loop.run_until_complete(self.process_adaptive())
        except Exception as e:
            print(f"[CRITICAL ERROR] {e}")
            import traceback
            traceback.print_exc()
            self.finished_error.emit(str(e))
        finally:
            self.loop.close()
            print("=== PROCESS FINISHED ===\n")

    def enforce_dictionary(self, text):
        """××‘×¦×¢ ××ª ×”×—×œ×¤×•×ª ×”××™×œ×•×Ÿ"""
        if not self.settings: return text
        nikud_dict = self.settings.get("nikud_dictionary", {})
        metadata = self.settings.get("nikud_metadata", {})
        sorted_keys = sorted(nikud_dict.keys(), key=len, reverse=True)
        processed_text = text
        for base_word in sorted_keys:
            target = nikud_dict[base_word]
            match_type = metadata.get(base_word, {}).get("match_type", "partial")
            if match_type == "exact":
                pattern = r'(?<![\w\u0590-\u05FF])' + re.escape(base_word) + r'(?![\w\u0590-\u05FF])'
                processed_text = re.sub(pattern, target, processed_text)
            else:
                processed_text = processed_text.replace(base_word, target)
        return processed_text

    def report_status(self):
        active_list = sorted(list(self.processing_idxs))
        msg = f"ğŸš€ ×¢×•×‘×“×™×: {self.current_limit} | ×¤×¢×™×œ: {active_list[:5]}..."
        self.log_update.emit(msg)

    async def process_adaptive(self):
        print("[DEBUG] TTSWorker: process_adaptive started (Preserving Layout)")
        
        # 1. × ×™×§×•×™ ×•×”×›× ×” - ×‘×œ×™ ×œ××—×•×§ ×™×¨×™×“×•×ª ×©×•×¨×”!
        text_processed = self.enforce_dictionary(self.text)
        text_processed = re.sub(r'(\d)\.(\d)', r'\1__DECIMAL_POINT__\2', text_processed)
        # ××¡×™×¨×™× ×¨×§ ××§×¤×™× ×‘×¢×™×™×ª×™×™×, ××‘×œ ××©××™×¨×™× ××ª \n
        text_processed = text_processed.replace("-", " ").replace("â€“", " ").replace("â€¢", "")
        
        # 2. ×—×œ×•×§×” ×—×›××” ×©×©×•××¨×ª ×¢×œ ×”××‘× ×”
        # ×”×‘×™×˜×•×™ ×”×–×” ××¤×¨×™×“: ×ª×’×™×•×ª, ×¡×™×× ×™ ×¡×™×•× ××©×¤×˜, ××• ×™×¨×™×“×•×ª ×©×•×¨×”
        split_pattern = r'(\[PAGE:\d+\]|\[IMG:.*?\]|[.?!]+|\n)'
        raw_parts = re.split(split_pattern, text_processed)
        
        final_sentences = []
        current_buffer = ""
        
        for part in raw_parts:
            if not part: continue
            
            part = part.replace("__DECIMAL_POINT__", ".")
            
            # ×˜×™×¤×•×œ ×‘×ª×’×™×•×ª (×¢××•×“/×ª××•× ×”) - ×¡×•×’×¨×™× ××ª ×”××©×¤×˜ ×”×§×•×“× ×•××•×¡×™×¤×™× ××ª ×”×ª×’×™×ª
            if "[PAGE:" in part or "[IMG:" in part:
                if current_buffer.strip(): # ×× × ×¦×‘×¨ ×˜×§×¡×˜, ×“×—×•×£ ××•×ª×• ×œ×¨×©×™××”
                    final_sentences.append(current_buffer)
                    current_buffer = ""
                final_sentences.append(part)
                continue
            
            # ×¦×‘×™×¨×ª ×”×˜×§×¡×˜ ×œ×ª×•×š ×”×‘××¤×¨ ×”× ×•×›×—×™
            current_buffer += part
            
            # ×‘×“×™×§×”: ×”×× ×”×’×¢× ×• ×œ×¡×•×£ "×™×—×™×“×”"?
            # ×™×—×™×“×” ×”×™× ×¡×•×£ ××©×¤×˜ (.) ××• ×¡×•×£ ×©×•×¨×” (\n)
            is_newline = "\n" in part
            is_punctuation = re.match(r'^[.?!]+$', part.strip())
            
            if is_newline or is_punctuation:
                # ×× ×™×© ×œ× ×• ×˜×§×¡×˜ ×××©×™ ×‘×‘××¤×¨ - ×©×•××¨×™× ××•×ª×• ×›××©×¤×˜
                if current_buffer.strip():
                    final_sentences.append(current_buffer)
                    current_buffer = ""
                
                # ××§×¨×” ×§×¦×”: ×©×•×¨×•×ª ×¨×™×§×•×ª (×¨×§ \n)
                # ×× ×”×‘××¤×¨ ××›×™×œ ×¨×§ ×™×¨×™×“×ª ×©×•×¨×” (×‘×œ×™ ×˜×§×¡×˜), ×× ×—× ×• ×¨×•×¦×™× ×œ×¦×¨×£ ××•×ª×” ×œ××©×¤×˜ ×”×§×•×“×
                # ×›×“×™ ×©×”× ×’×Ÿ ×™×“×¢ ×œ×¨×“×ª ×©×•×¨×” ××—×¨×™ ×”××©×¤×˜ ×”×§×•×“×
                elif is_newline and current_buffer == "\n":
                    if final_sentences and not final_sentences[-1].startswith("["):
                         final_sentences[-1] += "\n"
                    current_buffer = ""

        # ×©××¨×™×•×ª ×©× ×©××¨×• ×‘×‘××¤×¨
        if current_buffer:
            final_sentences.append(current_buffer)

        self.total_sentences = len(final_sentences)
        print(f"[DEBUG] Total items to process: {self.total_sentences}")

        # === ×©×œ×‘ ×”×¢×™×‘×•×“ (Processing) ===
        queue = asyncio.Queue()
        
        # ××™×•×Ÿ: ×¢××•×“×™× ×•×ª××•× ×•×ª ×œ× × ×©×œ×—×™× ×œ-TTS
        for i, s in enumerate(final_sentences):
            # ×× ×–×” ×œ× ×˜×§×¡×˜ ×œ×”×§×¨××” (×¢××•×“, ×ª××•× ×”, ××• ×¨×§ ×™×¨×™×“×ª ×©×•×¨×”)
            if "[PAGE:" in s or "[IMG:" in s or not s.strip():
                # ××¡×× ×™× ×›×”×¦×œ×—×” ×¨×™×§×” (×©×§×˜)
                self.results[i] = AudioSegment.silent(duration=0) 
            else:
                queue.put_nowait((i, s))

        # ×”×¨×¦×ª ×”-Workers
        active_tasks = set()
        
        while not queue.empty() or active_tasks:
            while len(active_tasks) < self.current_limit and not queue.empty():
                idx, sent = queue.get_nowait()
                # ×©×•×œ×—×™× ××ª ×”×˜×§×¡×˜ × ×§×™ ××¨×•×•×—×™× ××™×•×ª×¨×™× ×œ-TTS, ××‘×œ ×©×•××¨×™× ××ª ×”××§×•×¨ ×œ-JSON
                clean_sent_for_tts = sent.strip()
                task = asyncio.create_task(self.worker_wrapper(idx, clean_sent_for_tts))
                active_tasks.add(task)

            if active_tasks:
                done, pending = await asyncio.wait(active_tasks, return_when=asyncio.FIRST_COMPLETED)
                for task in done:
                    active_tasks.remove(task)
                    try:
                        result = await task
                        if result['status'] == 'success':
                            self.results[result['index']] = result['audio']
                            self.completed_count += 1
                            progress = int((self.completed_count / self.total_sentences) * 100)
                            self.progress_update.emit(progress)
                    except Exception: pass

        # === ×©×œ×‘ ×”×©××™×¨×” (Saving) ===
        self.log_update.emit("×‘×•× ×” ×§×•×‘×¥ ×¡×•×¤×™...")
        
        final_audio = AudioSegment.empty()
        karaoke_data = [] 
        current_time_ms = 0
        pause_duration = self.settings.get("pause_sentence", 600)
        silence_segment = AudioSegment.silent(duration=pause_duration, frame_rate=24000)

        for i in range(self.total_sentences):
            raw_text = final_sentences[i]
            
            # 1. ×˜×™×¤×•×œ ×‘×˜×¨×™×’×¨ ×¢××•×“
            if "[PAGE:" in raw_text:
                try:
                    match = re.search(r'\[PAGE:(\d+)\]', raw_text)
                    if match:
                        page_num = int(match.group(1))
                        karaoke_data.append({
                            "index": i, "text": "", "start": current_time_ms, "end": current_time_ms,
                            "is_image": False, "page_trigger": page_num
                        })
                except: pass
                continue 

            # 2. ×˜×™×¤×•×œ ×‘××•×“×™×•
            if i in self.results:
                segment = self.results[i]
                if segment.frame_rate != 24000: segment = segment.set_frame_rate(24000)
                
                duration = len(segment)
                
                # ×›××Ÿ ×”×§×¡×: raw_text ××›×™×œ ××ª ×”-\n ×”××§×•×¨×™ ××”××“×™×˜×•×¨
                karaoke_data.append({
                    "index": i,
                    "text": raw_text, # <--- ×”×˜×§×¡×˜ ×”××§×•×¨×™ ×¢× ×™×¨×™×“×•×ª ×”×©×•×¨×” × ×©××¨
                    "start": current_time_ms,
                    "end": current_time_ms + duration,
                    "is_image": "[IMG:" in raw_text
                })
                
                final_audio += segment
                current_time_ms += duration
                
                if "[IMG:" not in raw_text and raw_text.strip():
                    final_audio += silence_segment
                    current_time_ms += pause_duration

        # ×™×™×¦×•× ×œ×§×•×‘×¥
        try:
            await self.loop.run_in_executor(None, final_audio.export, self.output_path, "mp3")
            
            json_path = self.output_path.replace(".mp3", ".json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(karaoke_data, f, ensure_ascii=False, indent=4)
                
            self.finished_success.emit(self.output_path, self.skipped_sentences)
            
        except Exception as e:
            self.finished_error.emit(f"×©×’×™××” ×‘×©××™×¨×”: {e}")

    async def worker_wrapper(self, idx, sentence):
        self.processing_idxs.add(idx)
        self.report_status()
        try:
            audio = await self.process_single_sentence(sentence, idx)
            if len(audio) > 0:
                self.consecutive_errors = 0
                return {'status': 'success', 'index': idx, 'audio': audio}
            else:
                raise ServerOverloadError("Empty audio")
        except Exception as e:
            # ×‘××§×¨×” ×©×œ ×›×™×©×œ×•×Ÿ ××•×—×œ×˜ ××—×¨×™ × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™×
            print(f"[ERROR] Worker failed on {idx}: {e}")
            # ××—×–×™×¨×™× ×©×§×˜ ×›×“×™ ×œ× ×œ×ª×§×•×¢ ××ª ×”×ª×”×œ×™×š
            return {'status': 'success', 'index': idx, 'audio': AudioSegment.silent(duration=500, frame_rate=24000)}
        finally:
            if idx in self.processing_idxs: self.processing_idxs.remove(idx)

    async def process_single_sentence(self, sentence, idx):
        if "[IMG:" in sentence:
            return AudioSegment.silent(duration=4000, frame_rate=24000)
            
        try:
             # ×©×™××•×© ×‘×œ×•×’×™×§×” ×”×§×™×™××ª (Dual Mode ××• ×¨×’×™×œ)
             if self.dual_mode:
                 return await self.generate_natural_audio(sentence, idx)
             else:
                 audio_bytes = await self.fetch_audio_internal(sentence, self.he_voice, idx)
                 if audio_bytes:
                    return await self.loop.run_in_executor(None, self.bytes_to_audio, audio_bytes, idx)
                 return AudioSegment.silent(duration=0, frame_rate=24000)
        except Exception as e: 
            print(f"[RETRY NEEDED] {e}")
            # ×›××Ÿ ×××•×¨×” ×œ×”×™×•×ª ×œ×•×’×™×§×ª ×”-Retry ×”××œ××” ×©×œ×š (×§×™×¦×¨×ª×™ ×œ××¢×Ÿ ×”×‘×”×™×¨×•×ª)
            # ×× ×™×© ×œ×š ××ª ×”×¤×•× ×§×¦×™×” retry_logic, ×ª×§×¨× ×œ×” ×›××Ÿ
            return AudioSegment.silent(duration=0, frame_rate=24000)

    async def fetch_audio_internal(self, text, voice, log_id):
        if not text or not text.strip(): return None
        try:
            communicate = edge_tts.Communicate(text, voice, rate=self.speed)
            data = b""
            async for chunk in communicate.stream():
                if chunk["type"] == "audio": data += chunk["data"]
            return data
        except Exception as e:
            print(f"[EDGE-TTS ERROR] {e}")
            return None

    def bytes_to_audio(self, audio_bytes, idx):
        try:
            seg = AudioSegment.from_file(io.BytesIO(audio_bytes), format="mp3")
            # ×•×™×“×•× ×ª×“×¨ ×“×’×™××” ××—×™×“
            if seg.frame_rate != 24000: seg = seg.set_frame_rate(24000)
            return self.smart_trim(seg, idx)
        except: return AudioSegment.silent(duration=0, frame_rate=24000)

    def smart_trim(self, audio_segment, idx, limit=-40):
        """×—×•×ª×š ×©×§×˜ ××ª×—×™×œ×ª ×•×¡×•×£ ×§×˜×¢ ××•×“×™×•"""
        from pydub.silence import detect_leading_silence
        try:
            start_trim = detect_leading_silence(audio_segment, silence_threshold=limit)
            end_trim = detect_leading_silence(audio_segment.reverse(), silence_threshold=limit)
            duration = len(audio_segment)
            if start_trim + end_trim >= duration:
                return audio_segment
            return audio_segment[start_trim:duration - end_trim]
        except Exception:
            return audio_segment

    def _merge_language_parts(self, raw_parts):
        """
        ×××—×“ ×—×œ×§×™× ×§×˜× ×™× (×¡×™×× ×™×, ×¡×•×’×¨×™×™×, ××•×ª×™×•×ª ×‘×•×“×“×•×ª) ×¢× ×”×—×œ×§ ×”×¡××•×š,
        ×•×××—×“ ×—×œ×§×™× ×¨×¦×•×¤×™× ×‘××•×ª×” ×©×¤×” ×œ×—×œ×§ ××—×“.
        ××•× ×¢ ×©×‘×™×¨×” ×©×œ × ×•×¡×—××•×ª ×›××• Qâˆ—Eâˆ—C=Q(C-Cout) ×œ×—×œ×§×™× ×–×¢×™×¨×™×.
        """
        # ×©×œ×‘ 1: ×¡×™× ×•×Ÿ ×—×œ×§×™× ×¨×™×§×™×
        stripped = [(p, p.strip()) for p in raw_parts if p.strip()]
        if not stripped:
            return []

        # ×©×œ×‘ 2: ××™×—×•×“ ×¡×™×× ×™× ×‘×œ×‘×“ ×¢× ×”×©×›× ×™×
        merged = []
        for original, s in stripped:
            has_hebrew = bool(re.search(r'[\u0590-\u05FF]', s))
            has_latin = bool(re.search(r'[A-Za-z]', s))
            is_symbols_only = not has_hebrew and not has_latin

            if is_symbols_only and merged:
                merged[-1] = merged[-1] + original
            elif is_symbols_only and not merged:
                merged.append(original)
            else:
                if merged:
                    prev = merged[-1].strip()
                    prev_has_hebrew = bool(re.search(r'[\u0590-\u05FF]', prev))
                    prev_has_latin = bool(re.search(r'[A-Za-z]', prev))
                    if not prev_has_hebrew and not prev_has_latin:
                        merged[-1] = merged[-1] + original
                        continue
                merged.append(original)

        # ×©×œ×‘ 3: ××™×—×•×“ ×—×œ×§×™× ×¨×¦×•×¤×™× ×‘××•×ª×” ×©×¤×”
        consolidated = []
        for part in merged:
            s = part.strip()
            if not s:
                continue
            has_hebrew = bool(re.search(r'[\u0590-\u05FF]', s))
            has_latin = bool(re.search(r'[A-Za-z]', s))
            current_lang = "he" if has_hebrew else ("en" if has_latin else "symbols")

            if consolidated:
                prev_text = consolidated[-1].strip()
                prev_has_hebrew = bool(re.search(r'[\u0590-\u05FF]', prev_text))
                prev_has_latin = bool(re.search(r'[A-Za-z]', prev_text))
                prev_lang = "he" if prev_has_hebrew else ("en" if prev_has_latin else "symbols")

                if current_lang == prev_lang or current_lang == "symbols" or prev_lang == "symbols":
                    consolidated[-1] = consolidated[-1] + part
                    continue

            consolidated.append(part)

        # ×©×œ×‘ 4: ×¡×™× ×•×Ÿ ×¡×•×¤×™
        result = []
        for part in consolidated:
            s = part.strip()
            if s:
                result.append(s)
        return result

    async def generate_natural_audio(self, sentence, idx):
        """
        ××¤×¦×œ ××©×¤×˜ ×œ×¤×™ ×©×¤×” (×¢×‘×¨×™×ª/×× ×’×œ×™×ª) ×•××™×™×¦×¨ ××•×“×™×• ×¢× ×”×§×•×œ ×”××ª××™× ×œ×›×œ ×—×œ×§.
        ×—×•×ª×š ×©×§×˜ ××§×¦×•×•×ª ×›×œ ×§×˜×¢ ×•××•×¡×™×£ ×”×¤×¡×§×” ××‘×•×§×¨×ª (pause_lang) ×‘×™×Ÿ ×—×™×œ×•×¤×™ ×©×¤×”.
        """
        # ×¤×™×¦×•×œ: ×ª×•×¤×¡ ×¨×¦×¤×™ ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª ×›×•×œ×œ ×¡×¤×¨×•×ª ××¢×•×¨×‘×•×ª (OATP1B1, CYP3A4, P450)
        raw_parts = re.split(r'([A-Za-z][A-Za-z0-9\s\'\-\.]*[A-Za-z0-9]|[A-Za-z]+)', sentence)

        # ××™×—×•×“ ×—×œ×§×™× ×§×˜× ×™× (×¡×™×× ×™×, ×¡×•×’×¨×™×™×) ×¢× ×”×©×›× ×™× ×©×œ×”×
        parts = self._merge_language_parts(raw_parts)

        combined = AudioSegment.empty()
        has_audio = False
        prev_lang = None

        # ×”×¤×¡×§×” ××‘×•×§×¨×ª ×‘×™×Ÿ ×—×™×œ×•×¤×™ ×©×¤×”
        pause_lang = self.settings.get("pause_lang", 80)
        lang_silence = AudioSegment.silent(duration=pause_lang, frame_rate=24000) if pause_lang > 0 else None

        for part_stripped in parts:
            if not part_stripped:
                continue

            # ×‘×“×™×§×” ×× ×”×—×œ×§ ××›×™×œ ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª
            is_english = bool(re.search(r'[A-Za-z]', part_stripped))
            voice = self.en_voice if is_english else self.he_voice
            current_lang = "en" if is_english else "he"

            audio_bytes = await self.fetch_audio_internal(part_stripped, voice, idx)
            if audio_bytes:
                segment = await self.loop.run_in_executor(None, self.bytes_to_audio, audio_bytes, idx)
                if len(segment) > 0:
                    # ×”×•×¡×¤×ª ×”×¤×¡×§×” ××‘×•×§×¨×ª ×¨×§ ×›×©×™×© ×—×™×œ×•×£ ×©×¤×” ×‘×¤×•×¢×œ
                    if has_audio and lang_silence and prev_lang is not None and prev_lang != current_lang:
                        combined += lang_silence
                    combined += segment
                    has_audio = True
                    prev_lang = current_lang

        if has_audio:
            return combined
        return AudioSegment.silent(duration=0, frame_rate=24000)


class AudioPreviewWorker(QThread):
    # ×”××•×ª ××—×–×™×¨ ×›×¢×ª ×©× ×™ ×“×‘×¨×™×: ××ª ×”××¤×ª×— ×”×™×™×—×•×“×™ ×•××ª ×”××™×“×¢ ×¢×¦××•
    finished_data = pyqtSignal(str, bytes) 

    def __init__(self, cache_key, text, voice, speed):
        super().__init__()
        self.cache_key = cache_key
        self.text = text
        self.voice = voice
        self.speed = speed

    def run(self):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(self.generate())
        loop.close()

    async def generate(self):
        try:
            data = b""
            communicate = edge_tts.Communicate(self.text, self.voice, rate=self.speed)
            
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    data += chunk["data"]
            
            # ×”×—×–×¨×ª ×”××¤×ª×— ×•×”××™×“×¢
            self.finished_data.emit(self.cache_key, data)
            
        except Exception as e:
            print(f"Preview Memory Error: {e}")
